{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "import cv2\n",
    "from skimage.measure import label\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Segmentator():\n",
    "    def __init__(self):\n",
    "        self.lines = []\n",
    "        self.words = []\n",
    "        self.chars = []\n",
    "    def lines_seg(self,image):\n",
    "        image = 255 - image \n",
    "        lined = np.sum(image, 1)\n",
    "        threshold = 10\n",
    "        lines_ind = []\n",
    "        inertion = 0\n",
    "        cut = False\n",
    "        k1 = 0\n",
    "        k2 = 0\n",
    "        for i in range(lined.size):   \n",
    "            if (lined[i] > threshold)and(not(cut)):\n",
    "                k1 = i - 4\n",
    "                cut = True\n",
    "                inertion = 10    \n",
    "            if (lined[i] < threshold)and(cut)and(inertion == 0):\n",
    "                k2 = i + 4\n",
    "                cut = False\n",
    "                lines_ind.append((k1,k2))\n",
    "            inertion = max(0, inertion - 1)\n",
    "        return lines_ind\n",
    "\n",
    "\n",
    "\n",
    "    def words_seg(self,line):\n",
    "        line = 255 - line\n",
    "        size = line.shape[1]\n",
    "        words_ind = []\n",
    "        words_hist = np.sum(line, 0)\n",
    "        threshold = 0\n",
    "        inertion = 0\n",
    "        cut = False\n",
    "        k1 = 0\n",
    "        k2 = 0\n",
    "        for i in range(words_hist.size):\n",
    "            place = words_hist[i]\n",
    "            for k in range(1):\n",
    "                place += words_hist[(i+k)%size]\n",
    "            if (place > threshold)and(not(cut)):\n",
    "                k1 = i\n",
    "                cut = True  \n",
    "            if (place <= threshold)and(cut):\n",
    "                k2 = i\n",
    "                cut = False\n",
    "                if (k2 - k1 > 5):\n",
    "                    words_ind.append((k1,k2))\n",
    "        return words_ind\n",
    "\n",
    "    def check_common(self,x,y):\n",
    "        filt1 = np.sum(x,0) > 0\n",
    "        filt2 = np.sum(y,0) > 0\n",
    "        if np.sum(filt1*filt2) > 1:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def chars_seg(self,word):\n",
    "        word = 255 - word\n",
    "        skip = False\n",
    "        labeled_word = label(word)\n",
    "        labels = list(set(labeled_word.flatten()))\n",
    "        labels.remove(0)\n",
    "        if len(labels) == 1:\n",
    "            return word\n",
    "        chars_temp = []\n",
    "        chars = []\n",
    "        overflow = 0\n",
    "        filters = []\n",
    "        for i in range(len(labels)):\n",
    "            chars_temp.append(np.zeros(shape=word.shape))\n",
    "            filtering = labeled_word ==labels[i]\n",
    "            chars_temp[i] = 255*filtering\n",
    "        i = 0\n",
    "        steps = len(labels) \n",
    "        while i < steps:\n",
    "            j = i+1\n",
    "            skip = False\n",
    "            while (j < steps)and(not(skip)):\n",
    "                if self.check_common(chars_temp[i], chars_temp[j]):\n",
    "                    filters.append((chars_temp[i] ==255) + (chars_temp[j] ==255))\n",
    "                    steps = steps - 1\n",
    "                    labels.pop(j)\n",
    "                    skip = True\n",
    "                    i += 1\n",
    "                j += 1\n",
    "            if not(skip):\n",
    "                filters.append((labeled_word ==labels[i]))\n",
    "                i += 1\n",
    "        for i in range(len(filters)):\n",
    "            temp = filters[i]*255\n",
    "            temp = self.crop_char(temp)\n",
    "            temp = self.expand_char(temp,30)\n",
    "            chars.append(temp)\n",
    "        return chars\n",
    "    \n",
    "    def expand_char(self,char,size):\n",
    "        arr = np.zeros(shape = (size,size))\n",
    "        arr = 255 - arr\n",
    "        char_sized = char[:size,:size]\n",
    "        height = char.shape[0]\n",
    "        width= char.shape[1]\n",
    "        for i in range(size):\n",
    "            if i < height:\n",
    "                for j in range(size):\n",
    "                    if j < width:\n",
    "                        arr[i][j+6] = char[i][j]\n",
    "        return arr\n",
    "\n",
    "    def crop_char(self,char):\n",
    "        left = 0\n",
    "        size = 30\n",
    "        right = char.shape[1] - 1\n",
    "        bottom = char.shape[0] - 1\n",
    "        top = 0\n",
    "        stop = False\n",
    "        ver = np.sum(char, 1)\n",
    "        hor = np.sum(char, 0)\n",
    "        for i in range(char.shape[0]):\n",
    "            if (ver[i] <> 0)and(not(stop)):\n",
    "                stop = True\n",
    "                top = i\n",
    "            if (ver[i] == 0)and(stop)and(i > size/2):\n",
    "                bottom = i\n",
    "                stop = False\n",
    "        for i in range(char.shape[1]):\n",
    "            if (hor[i] <> 0)and(not(stop)):\n",
    "                stop = True\n",
    "                left = i\n",
    "            if (hor[i] == 0) and(stop):\n",
    "                right = i\n",
    "                stop = False \n",
    "        return self.expand_char(char[top:bottom,left:right],size)\n",
    "    \n",
    "    def segment(self,image_path):\n",
    "        chars = []\n",
    "        l = 0\n",
    "        image = cv2.imread(image_path, 0)\n",
    "        ret, image = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "        lines_ind = self.lines_seg(image)\n",
    "        for i in range(len(lines_ind)):\n",
    "            line = image[lines_ind[i][0]:lines_ind[i][1]]\n",
    "            self.lines.append(line)\n",
    "            chars.append(list())\n",
    "            words_ind = self.words_seg(line)\n",
    "            for j in range(len(words_ind)):\n",
    "                chars[i].append(list())\n",
    "                word = line[:,words_ind[j][0]:words_ind[j][1]]\n",
    "                chars[i][j].append(self.expand_char(word,30))\n",
    "                self.chars.append(self.expand_char(word,30))\n",
    "        return chars\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "segg = Segmentator()\n",
    "pa = '../resources/lete.jpg'\n",
    "char = segg.segment(pa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Batch_generator:\n",
    "    def __init__(self, chars):\n",
    "        self.chars = chars\n",
    "        self.cursor = 0\n",
    "        self.spaces =[]\n",
    "        self.nline = 0\n",
    "        self.charlist = segg.chars\n",
    "        space = 0\n",
    "        for i in range(len(self.chars)):\n",
    "            for j in range(len(self.chars[i])):\n",
    "                for k in range(len(self.chars[i][j])):\n",
    "                    space +=1\n",
    "                self.spaces.append(space)\n",
    "                \n",
    "    def generate_train_batch(self):\n",
    "        \n",
    "        labels = []\n",
    "        train_chars = []\n",
    "        for i in range(32):\n",
    "            k = random.randint(0,voc_size-1)\n",
    "            label = np.zeros(shape=(1,68))\n",
    "            label[0,k] = 1.0\n",
    "            labels.append(label)\n",
    "            train_char = cv2.imread('../resources/dataset/' + str(k) +'/'+str(k)+'_0.png',0)\n",
    "            train_chars.append(np.reshape(train_char, (1,900)))\n",
    "        return train_chars, labels\n",
    "        \n",
    "        \n",
    "        \n",
    "    def generate_sample_batch(self):\n",
    "        voc_size = len(letters)\n",
    "        batch = np.reshape(self.charlist[self.cursor%len(self.charlist)], (1,900))\n",
    "        self.cursor = (self.cursor + 1)%len(self.charlist)\n",
    "        return 255 - batch\n",
    "    \n",
    "bagen = Batch_generator(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "letters = '`bdfhijkIABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890)(taceglnopqrsuvwxyz-,.'\n",
    "voc_size = len(letters)\n",
    "size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "voc_size = len(letters)\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    W = tf.Variable(tf.truncated_normal([size**2,voc_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([voc_size]))\n",
    "    train_inputs = []\n",
    "    train_labels = []\n",
    "    for i in range(32):\n",
    "        train_inputs.append(tf.placeholder(tf.float32, shape=[1,size**2]))\n",
    "        train_labels.append(tf.placeholder(tf.float32, shape=[1,voc_size]))\n",
    "    \n",
    "    logits = tf.nn.xw_plus_b(tf.concat(train_inputs,0), W, b)\n",
    "    loss = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(\n",
    "                    logits=logits, labels=tf.concat(train_labels,0)))\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        0.1, global_step, 500, 0.65, staircase=False)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 5)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1,size**2])\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_input,W,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 406.593994\n",
      "Average loss at step 500: 13.206969\n",
      "Average loss at step 1000: 4.527713\n",
      "Average loss at step 1500: 2.358395\n",
      "Average loss at step 2000: 1.766281\n",
      "Average loss at step 2500: 0.980532\n",
      "5B0KrkKDpOpy(UV\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3000\n",
    "summary_frequency = 500\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batch, labels = bagen.generate_train_batch()\n",
    "        feed_dict = dict()\n",
    "        for i in range(32):\n",
    "            \n",
    "            feed_dict[train_inputs[i]] = batch[i]\n",
    "            feed_dict[train_labels[i]] = labels[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            print('Average loss at step %d: %f' % (step, mean_loss)) \n",
    "        if (step == 2800):\n",
    "            text = ''\n",
    "            ka = []\n",
    "            for i in range(15):\n",
    "                sample_feed_dict = dict()\n",
    "                ka.append(bagen.generate_sample_batch())\n",
    "                sample_feed_dict[sample_input] = ka[i]\n",
    "                prediction = sample_prediction.eval(feed_dict=sample_feed_dict)\n",
    "                text += letters[np.argmax(prediction)]\n",
    "            print(text)\n",
    "  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f5f84dab590>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxi819T9I/0gC08j66gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5f7c3f6c90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.reshape(ka[0], (30,30)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'V'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letters[np.argmax(prediction)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
